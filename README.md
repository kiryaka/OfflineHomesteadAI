# Universe - Hybrid Search System

A powerful hybrid search system combining text search (Tantivy) and vector search (LanceDB) for processing and searching large document collections.

## ğŸ—ï¸ Architecture

```
Raw Documents â†’ ELT Pipeline â†’ Processed Data â†’ Search System
```

- **ELT Pipeline** (Python): Extract, Load, Transform documents
- **Search System** (Rust): Fast text and vector search

## ğŸ“ Project Structure

```
universe/
â”œâ”€â”€ search/                    # Rust search system
â”‚   â”œâ”€â”€ src/                   # Source code
â”‚   â”œâ”€â”€ tests/                 # Tests
â”‚   â”œâ”€â”€ Cargo.toml            # Rust dependencies
â”‚   â””â”€â”€ config*.toml          # Configuration files
â”‚
â”œâ”€â”€ etl/                      # Python data processing pipeline
â”‚   â”œâ”€â”€ src/                  # Source code
â”‚   â”‚   â”œâ”€â”€ extractors/       # Document extractors
â”‚   â”‚   â”œâ”€â”€ processors/       # Text processors
â”‚   â”‚   â””â”€â”€ loaders/          # Data loaders
â”‚   â”œâ”€â”€ config/               # Configuration
â”‚   â”œâ”€â”€ requirements.txt      # Python dependencies
â”‚   â””â”€â”€ pyproject.toml        # Python project config
â”‚
â”œâ”€â”€ data/                     # Shared data directory
â”‚   â”œâ”€â”€ raw/                  # Raw documents
â”‚   â”œâ”€â”€ processed/            # Clean text files
â”‚   â”œâ”€â”€ chunks/               # Chunked text
â”‚   â”œâ”€â”€ embeddings/           # Vector embeddings
â”‚   â””â”€â”€ indexes/              # Search indexes
â”‚
â”œâ”€â”€ scripts/                  # Utility scripts
â”‚   â”œâ”€â”€ setup.sh             # Initial setup
â”‚   â”œâ”€â”€ run_etl_pipeline.sh  # Run data processing
â”‚   â””â”€â”€ run_search.sh        # Run search system
â”‚
â””â”€â”€ docs/                     # Documentation
```

## ğŸš€ Quick Start

### 1. Setup

```bash
# Clone and setup
git clone <repository>
cd universe
./scripts/setup.sh
```

### 2. Add Documents

```bash
# Add your documents to data/raw/
cp your_documents/* data/raw/
```

### 3. Process Documents

```bash
# Run the complete ELT pipeline
./scripts/run_etl_pipeline.sh
```

### 4. Start Search System

```bash
# Start the search server
./scripts/run_search.sh
```

## ğŸ”§ Configuration

### Search System (Rust)
- `search/config.dev.toml` - Development settings
- `search/config.prod.toml` - Production settings

### ELT Pipeline (Python)
- `etl/config/etl_config.yaml` - Processing settings

## ğŸ“Š Features

### ELT Pipeline
- **Document Extraction**: PDF, DOCX, images, and more
- **Text Processing**: Smart chunking and tokenization
- **Embedding Generation**: Vector embeddings for semantic search
- **Data Export**: Clean data for search systems

### Search System
- **Text Search**: Fast full-text search with Tantivy
- **Vector Search**: Semantic search with LanceDB
- **Hybrid Search**: Combine text and vector results
- **Environment Configs**: Separate dev/prod settings

## ğŸ› ï¸ Development

### Rust Search System
```bash
cd search
cargo test                    # Run tests
cargo run --bin lancedb_production_example  # Run production example
```

### Python ELT Pipeline
```bash
cd etl
pip install -r requirements.txt
python -m src.cli --help      # See available commands
```

## ğŸ“ˆ Performance

- **Development**: Fast iteration with smaller indexes
- **Production**: Optimized for 100GB+ corpora
- **Vector Compression**: 16x compression with 98.4% storage reduction
- **Search Speed**: Sub-second search across large collections

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## ğŸ“„ License

MIT License - see LICENSE file for details.